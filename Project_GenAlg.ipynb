{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 16:35:21.942432: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-11 16:35:21.943998: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-11 16:35:21.969659: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-11 16:35:21.969692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-11 16:35:21.970409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-11 16:35:21.974752: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-11 16:35:21.975308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-11 16:35:22.531441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from numpy import random as rand\n",
    "import random\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "def build():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_fit(model, batch_size = 32, learning_rate = 0.001 , x_train = x_train, y_train = y_train, validation_data=(x_test, y_test)):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    val_accuracy_metric = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    @tf.function\n",
    "    def run_train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(labels, logits)\n",
    "            if model.losses:\n",
    "                loss += tf.math.add_n(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def run_val_step(images, labels):\n",
    "        logits = model(images)\n",
    "        val_accuracy_metric.update_state(labels, logits)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    timedelt = 0.0\n",
    "    i=0\n",
    "    init_time = datetime.now()\n",
    "    for epoch in range(2):\n",
    "        # print(f\"Epoch: {int(epoch)+1}\")\n",
    "\n",
    "        for images, labels in train_ds:\n",
    "            run_train_step(images, labels)\n",
    "\n",
    "        val_accuracy_metric.reset_states()\n",
    "\n",
    "        for images, labels in validation_data:\n",
    "            run_val_step(images, labels)\n",
    "\n",
    "        val_accuracy = float(val_accuracy_metric.result().numpy())\n",
    "\n",
    "        # print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        if val_accuracy < best_val_accuracy:\n",
    "            i = i + 1    \n",
    "            \n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "\n",
    "        if i > 3: # Early stopping criteria\n",
    "            break\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    timedelt = round((end_time - init_time).total_seconds(), 4)\n",
    "    \n",
    "\n",
    "    return float(best_val_accuracy), timedelt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_fit(model, batch_size = 32, learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.99, x_train = x_train, y_train = y_train, validation_data=(x_test, y_test)):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate = learning_rate,\n",
    "        beta_1 = beta_1,\n",
    "        beta_2 = beta_2,\n",
    "    )\n",
    "    \n",
    "    loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    val_accuracy_metric = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    @tf.function\n",
    "    def run_train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(labels, logits)\n",
    "            if model.losses:\n",
    "                loss += tf.math.add_n(model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def run_val_step(images, labels):\n",
    "        logits = model(images)\n",
    "        val_accuracy_metric.update_state(labels, logits)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    timedelt = 0.0\n",
    "    i=0\n",
    "    init_time = datetime.now()\n",
    "    for epoch in range(2):\n",
    "        # print(f\"Epoch: {int(epoch)+1}\")\n",
    "\n",
    "        for images, labels in train_ds:\n",
    "            run_train_step(images, labels)\n",
    "\n",
    "        val_accuracy_metric.reset_states()\n",
    "\n",
    "        for images, labels in validation_data:\n",
    "            run_val_step(images, labels)\n",
    "\n",
    "        val_accuracy = float(val_accuracy_metric.result().numpy())\n",
    "\n",
    "        # print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        if val_accuracy < best_val_accuracy:\n",
    "            i = i + 1    \n",
    "            \n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "\n",
    "        if i > 3: # Early stopping criteria\n",
    "            break\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    timedelt = end_time - init_time\n",
    "    \n",
    "\n",
    "    return best_val_accuracy, timedelt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GA Hyperparameters\n",
    "num_gens = 5\n",
    "population_num = 8 #Must be a number that is divisible by 4\n",
    "mutation_prob = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_GA:\n",
    "    def __init__(self, num_gens, num_pop, mut_prob):\n",
    "        self.num_gens = num_gens\n",
    "        self.num_pop = num_pop\n",
    "        self.mutation_chance = rand.poisson(lam=mut_prob, size=2*num_pop*num_gens)\n",
    "        self.genes = []\n",
    "        self.pop_eval = {}\n",
    "        self.ranked_keys = []\n",
    "        self.mutation_index = 0\n",
    "        \n",
    "    \n",
    "    def generate_population(self):\n",
    "        batch_sizes = 32+(32*rand.randint(low = 0, high = 15, size = self.num_pop)) #random multiples of 32 from 32 to 512\n",
    "        learning_rates = np.logspace(start = 0.0001, stop = 0.01, num = self.num_pop, endpoint=0.01)-1 # evenly spaced on a log scale, technically not random but random enough for my experiment\n",
    "        rand.shuffle(learning_rates)\n",
    "        \n",
    "        for i in range(self.num_pop):\n",
    "            self.genes.append((batch_sizes[i], learning_rates[i]))  \n",
    "            \n",
    "        \n",
    "    \n",
    "    def calculate_fitness(self):\n",
    "        self.pop_eval = {}\n",
    "        self.ranked_keys = []\n",
    "        for i in range(self.num_pop):\n",
    "            val_score, train_time = sgd_fit(model = build(), batch_size=self.genes[i][0], learning_rate=self.genes[i][1])\n",
    "            self.pop_eval[val_score] = (self.genes[i][0], self.genes[i][1], train_time)\n",
    "        \n",
    "    \n",
    "    def rankNelim(self):        \n",
    "        self.ranked_keys = list(self.pop_eval.keys())\n",
    "        self.ranked_keys.sort(reverse=True)\n",
    "        \n",
    "        self.genes = []\n",
    "        \n",
    "        #Populate with highest performing genes\n",
    "        for i in range(0,int(self.num_pop/4),1):\n",
    "            self.genes.append((self.pop_eval[self.ranked_keys[i]][0], self.pop_eval[self.ranked_keys[i]][1]))\n",
    "        \n",
    "    \n",
    "    def crossover(self):\n",
    "        #Populate with crossovers of highest performing genes(HARD CODED SO MUST BE ALTERED IF GENE STRUCTURE IS CHANGED[E.G. NEW HYPERPARAMETERS ARE INTRODUCED])\n",
    "        for i in range(0, int(self.num_pop/4), 2): # cross over every 2 genes' chromosomes and push it to the list of genes\n",
    "            self.genes.append((self.genes[i][0], self.genes[i+1][1]))\n",
    "            self.genes.append((self.genes[i+1][0], self.genes[i][1]))\n",
    "            \n",
    "        \n",
    "        #New values to replace low performing genes\n",
    "        batch_sizes = 32+(32*rand.randint(low = 0, high = 15, size = int(self.num_pop/2)))\n",
    "        learning_rates = np.logspace(start = 0.001, stop = 0.01, num = int(self.num_pop/2), endpoint=0.01)-1 \n",
    "        rand.shuffle(learning_rates)\n",
    "        \n",
    "        for i in range(int(self.num_pop/2)):\n",
    "            self.genes.append((batch_sizes[i], learning_rates[i]))\n",
    "        \n",
    "    \n",
    "    def mutate(self):\n",
    "        mut_gene = []\n",
    "        \n",
    "        for i in range(int(self.num_pop)):\n",
    "            for j in range(2):\n",
    "                if self.mutation_chance[self.mutation_index] > 0:\n",
    "                    if j == 0:\n",
    "                        mut_gene = list(self.genes[i])\n",
    "                        mut_gene[j] = 32+(32*rand.randint(low = 0, high = 15))\n",
    "                        self.genes.pop(i)\n",
    "                        self.genes.append(mut_gene)\n",
    "                        \n",
    "                    \n",
    "                    if j == 1:\n",
    "                        mut_gene = list(self.genes[i])\n",
    "                        mut_gene[j] = random.choice([0.95, 1.05]) * (self.genes[i][j])\n",
    "                        self.genes.pop(i)\n",
    "                        self.genes.append(mut_gene)\n",
    "                    \n",
    "                \n",
    "                self.mutation_index = self.mutation_index + 1\n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "    def GA(self):\n",
    "        print(\"Starting GA\\n\")\n",
    "        self.generate_population()\n",
    "        \n",
    "        for i in range(self.num_gens):\n",
    "            init_time = datetime.now()\n",
    "            print(f\"Generation {i+1} started\")\n",
    "            self.calculate_fitness()\n",
    "            self.rankNelim()\n",
    "            self.crossover()\n",
    "            self.mutate()\n",
    "            print(f\"Best\")\n",
    "            print(f\"Accuracy: {self.ranked_keys[0]}\")\n",
    "            print(f\"Batch Size: {self.pop_eval[self.ranked_keys[0]][0]}\")\n",
    "            print(f\"Learning Rate: {self.pop_eval[self.ranked_keys[0]][1]}\")\n",
    "            print(f\"Time: {self.pop_eval[self.ranked_keys[0]][2]}\\n\\n\\n\")\n",
    "            print(f\"Generation Time: {round((datetime.now()-init_time).total_seconds(), 4)}\\n\\n\\n\")\n",
    "        \n",
    "        self.calculate_fitness()\n",
    "        self.rankNelim()\n",
    "        self.mutation_index = 0\n",
    "        \n",
    "        \n",
    "        return self.ranked_keys[0], self.pop_eval[self.ranked_keys[0]][0], self.pop_eval[self.ranked_keys[0]][1], self.pop_eval[self.ranked_keys[0]][2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GA\n",
      "\n",
      "Generation 1 started\n",
      "Best\n",
      "Accuracy: 0.9714999794960022\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.016649914753727568\n",
      "Time: 23.9106\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 161.3771\n",
      "\n",
      "\n",
      "\n",
      "Generation 2 started\n",
      "Best\n",
      "Accuracy: 0.9757000207901001\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.019966045204601546\n",
      "Time: 24.3951\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 166.5048\n",
      "\n",
      "\n",
      "\n",
      "Generation 3 started\n",
      "Best\n",
      "Accuracy: 0.9754999876022339\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.019966045204601546\n",
      "Time: 24.5349\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 170.9543\n",
      "\n",
      "\n",
      "\n",
      "Generation 4 started\n",
      "Best\n",
      "Accuracy: 0.9757000207901001\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.0232929922807541\n",
      "Time: 24.1636\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 171.1751\n",
      "\n",
      "\n",
      "\n",
      "Generation 5 started\n",
      "Best\n",
      "Accuracy: 0.9782000184059143\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.0232929922807541\n",
      "Time: 24.1182\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 164.2066\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_ga = SGD_GA(num_pop = population_num, num_gens = num_gens, mut_prob = mutation_prob)\n",
    "top_val, top_batchsize, top_learningrate, top_traintime = sgd_ga.GA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9760000109672546\n",
      "32\n",
      "0.0232929922807541\n",
      "21.975\n",
      "{0.9760000109672546: (32, 0.0232929922807541, 21.975), 0.9754999876022339: (32, 0.020964347464831622, 23.2916), 0.9732999801635742: (32, 0.020964347464831622, 22.3228), 0.9746000170707703: (32, 0.0232929922807541, 24.803), 0.8787000179290771: (64, 0.0023052380778996184, 25.3439), 0.8959000110626221: (128, 0.009252886076684508, 19.9707), 0.8716999888420105: (480, 0.016248692870695525, 13.9792), 0.8888999819755554: (480, 0.022128342666716393, 13.6163)}\n"
     ]
    }
   ],
   "source": [
    "print(top_val)\n",
    "print(top_batchsize)\n",
    "print(top_learningrate)\n",
    "print(top_traintime)\n",
    "print(sgd_ga.pop_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaM_GA():\n",
    "    def __init__(self, num_gens, num_pop, mut_prob):\n",
    "        self.num_gens = num_gens\n",
    "        self.num_pop = num_pop\n",
    "        self.mutation_chance = rand.poisson(mut_prob, 4*num_pop*num_gens)\n",
    "        self.genes = []\n",
    "        self.pop_eval = {}\n",
    "        self.ranked_keys = []\n",
    "        self.mutation_index = 0\n",
    "    \n",
    "    def generate_population(self):\n",
    "        batch_sizes = 32+(32*rand.randint(low = 0, high = 15, size = self.num_pop)) #random multiples of 32 from 32 to 512\n",
    "        \n",
    "        learning_rates = np.logspace(start = 0.0001, stop = 0.01, num = self.num_pop, endpoint=0.01)-1 # evenly spaced on a log scale, technically not random but random enough for my experiment\n",
    "        rand.shuffle(learning_rates)\n",
    "        \n",
    "        beta_1 = np.logspace(start = 0.27, stop = 0.3, num = self.num_pop, endpoint=False)-1\n",
    "        rand.shuffle(beta_1)\n",
    "        \n",
    "        beta_2 = np.logspace(start = 0.29, stop = 0.3, num = self.num_pop, endpoint=False)-1\n",
    "        rand.shuffle(beta_2)\n",
    "        \n",
    "        for i in range(self.num_pop):\n",
    "            self.genes.append((batch_sizes[i], learning_rates[i], beta_1[i], beta_2[i]))  \n",
    "\n",
    "    \n",
    "    def calculate_fitness(self):\n",
    "        self.pop_eval = {}\n",
    "        self.ranked_keys = []\n",
    "        \n",
    "        for i in range(self.num_pop):\n",
    "            val_score, train_time = adam_fit(model = build(), batch_size=self.genes[i][0], learning_rate= self.genes[i][1], beta_1 = self.genes[i][2], beta_2 = self.genes[i][3])\n",
    "            self.pop_eval[val_score] = (self.genes[i][0], self.genes[i][1], self.genes[i][2], self.genes[i][3], train_time)\n",
    "    \n",
    "    def rankNelim(self):        \n",
    "        self.ranked_keys = list(self.pop_eval.keys())\n",
    "        self.ranked_keys.sort(reverse = True)\n",
    "        \n",
    "        self.genes = []\n",
    "        \n",
    "        #Populate with highest performing genes\n",
    "        for i in range(0, int(self.num_pop/4)):\n",
    "            self.genes.append((self.pop_eval[self.ranked_keys[i]][0], self.pop_eval[self.ranked_keys[i]][1], self.pop_eval[self.ranked_keys[i]][2], self.pop_eval[self.ranked_keys[i]][3]))\n",
    "\n",
    "    \n",
    "    def crossover(self):\n",
    "        #Crossover and append highest performing genes\n",
    "        for i in range(0, int(self.num_pop/4), 2): # cross over every 2 genes' chromosomes and push it to the list of genes\n",
    "            self.genes.append((self.genes[i][0], self.genes[i+1][1], self.genes[i][2], self.genes[i+1][3]))\n",
    "            self.genes.append((self.genes[i+1][0], self.genes[i][1], self.genes[i+1][2], self.genes[i][3]))\n",
    "        \n",
    "        #New values to replace low performing genes\n",
    "        batch_sizes = 32+(32*rand.randint(low = 0, high = 15, size = int(self.num_pop/2)))\n",
    "        \n",
    "        learning_rates = np.logspace(start = 0.001, stop = 0.01, num = int(self.num_pop/2), endpoint=False)-1 \n",
    "        rand.shuffle(learning_rates)\n",
    "        \n",
    "        beta_1 = np.logspace(start = 0.2, stop = 0.3, num = int(self.num_pop/2), endpoint=False)-1 \n",
    "        rand.shuffle(beta_1)\n",
    "        \n",
    "        beta_2 = np.logspace(start = 0.29, stop = 0.3, num = int(self.num_pop/2), endpoint=False)-1 \n",
    "        rand.shuffle(beta_2)\n",
    "        \n",
    "        for i in range(int(self.num_pop/2)):\n",
    "            self.genes.append((batch_sizes[i], learning_rates[i], beta_1[i], beta_2[i]))\n",
    "    \n",
    "    def mutate(self):\n",
    "        for i in range(int(self.num_pop)):\n",
    "            for j in range(4):\n",
    "                if self.mutation_chance[self.mutation_index] > 0:\n",
    "                    if j == 0:\n",
    "                        mut_gene = list(self.genes[i])\n",
    "                        mut_gene[j] = 32+(32*rand.randint(low = 0, high = 15))\n",
    "                        self.genes.pop(i)\n",
    "                        self.genes.append(mut_gene)\n",
    "                        \n",
    "                    \n",
    "                    if j == 1:\n",
    "                        mut_gene = list(self.genes[i])\n",
    "                        mut_gene[j] = random.choice([0.95, 1.05]) * (self.genes[i][j])\n",
    "                        self.genes.pop(i)\n",
    "                        self.genes.append(mut_gene)\n",
    "                    \n",
    "                \n",
    "                self.mutation_index = self.mutation_index + 1\n",
    "    \n",
    "    def GA(self):\n",
    "        self.generate_population()\n",
    "        print(\"Starting GA\\n\")\n",
    "        \n",
    "        for i in range(self.num_gens):\n",
    "            init_time = datetime.now()\n",
    "            print(f\"Generation {i+1} started\")\n",
    "            self.calculate_fitness()\n",
    "            self.rankNelim()\n",
    "            self.crossover()\n",
    "            self.mutate()\n",
    "            print(f\"Best\")\n",
    "            print(f\"Accuracy: {self.ranked_keys[0]}\")\n",
    "            print(f\"Batch Size: {self.pop_eval[self.ranked_keys[0]][0]}\")\n",
    "            print(f\"Learning Rate: {self.pop_eval[self.ranked_keys[0]][1]}\")\n",
    "            print(f\"Beta_1: {self.pop_eval[self.ranked_keys[0]][2]}\")\n",
    "            print(f\"Beta_2: {self.pop_eval[self.ranked_keys[0]][3]}\")\n",
    "            print(f\"Time: {self.pop_eval[self.ranked_keys[0]][4]}\\n\\n\\n\")\n",
    "            print(f\"Generation Time: {round((datetime.now()-init_time).total_seconds(), 4)}\\n\\n\\n\")\n",
    "        \n",
    "        self.calculate_fitness()\n",
    "        self.rankNelim()\n",
    "        self.mutation_index = 0\n",
    "        \n",
    "        return self.ranked_keys[0], self.pop_eval[self.ranked_keys[0]][0], self.pop_eval[self.ranked_keys[0]][1], self.pop_eval[self.ranked_keys[0]][2], self.pop_eval[self.ranked_keys[0]][3], self.pop_eval[self.ranked_keys[0]][4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GA\n",
      "\n",
      "Generation 1 started\n",
      "Best\n",
      "Accuracy: 0.9866999983787537\n",
      "Batch Size: 320\n",
      "Learning Rate: 0.0034928575720136745\n",
      "Beta_1: 0.9611011754760475\n",
      "Beta_2: 0.9781080029332396\n",
      "Time: 0:00:16.335079\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 161.0203\n",
      "\n",
      "\n",
      "\n",
      "Generation 2 started\n",
      "Best\n",
      "Accuracy: 0.987500011920929\n",
      "Batch Size: 320\n",
      "Learning Rate: 0.0034928575720136745\n",
      "Beta_1: 0.9611011754760475\n",
      "Beta_2: 0.9781080029332396\n",
      "Time: 0:00:15.913208\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 137.9849\n",
      "\n",
      "\n",
      "\n",
      "Generation 3 started\n",
      "Best\n",
      "Accuracy: 0.9868000149726868\n",
      "Batch Size: 320\n",
      "Learning Rate: 0.0075114721791962324\n",
      "Beta_1: 0.9611011754760475\n",
      "Beta_2: 0.9724227361148536\n",
      "Time: 0:00:16.067331\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 148.1605\n",
      "\n",
      "\n",
      "\n",
      "Generation 4 started\n",
      "Best\n",
      "Accuracy: 0.9887999892234802\n",
      "Batch Size: 320\n",
      "Learning Rate: 0.0034928575720136745\n",
      "Beta_1: 0.9611011754760475\n",
      "Beta_2: 0.9781080029332396\n",
      "Time: 0:00:16.731608\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 149.9056\n",
      "\n",
      "\n",
      "\n",
      "Generation 5 started\n",
      "Best\n",
      "Accuracy: 0.9868999719619751\n",
      "Batch Size: 416\n",
      "Learning Rate: 0.0034928575720136745\n",
      "Beta_1: 0.9611011754760475\n",
      "Beta_2: 0.9781080029332396\n",
      "Time: 0:00:14.690412\n",
      "\n",
      "\n",
      "\n",
      "Generation Time: 145.6419\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adam_ga = AdaM_GA(num_pop = population_num, num_gens = num_gens, mut_prob = mutation_prob)\n",
    "tops_val, tops_batchsize, tops_learningrate, top_beta1, top_beta2, tops_traintime = adam_ga.GA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9871000051498413\n",
      "320\n",
      "0.0034928575720136745\n",
      "0.9611011754760475\n",
      "0.9781080029332396\n",
      "0:00:16.815866\n"
     ]
    }
   ],
   "source": [
    "print(tops_val)\n",
    "print(tops_batchsize)\n",
    "print(tops_learningrate)\n",
    "print(top_beta1)\n",
    "print(top_beta2)\n",
    "print(tops_traintime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
